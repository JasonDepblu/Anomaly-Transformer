{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1Hrlq0_Q3BT9ZYK8w5x6jPdGg-isE_OkM",
      "authorship_tag": "ABX9TyMDrIMSi4wyp0UIFASZqLBS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonDepblu/Anomaly-Transformer/blob/main/misfire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "import os\n",
        "import argparse\n",
        "from torch.backends import cudnn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "SegU9918qIA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "A0skNuM5tFU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import weight_norm"
      ],
      "metadata": {
        "id": "vJwrthI8wDxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "LXvdmXOWv619"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import collections\n",
        "import numbers\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle"
      ],
      "metadata": {
        "id": "_UajJsOXwNJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_var(x, volatile=False):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x, volatile=volatile)\n",
        "\n",
        "\n",
        "def mkdir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)"
      ],
      "metadata": {
        "id": "eT9ryvrAtTg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TriangularCausalMask():\n",
        "    def __init__(self, B, L, device=\"cpu\"):\n",
        "        mask_shape = [B, 1, L, L]\n",
        "        with torch.no_grad():\n",
        "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        return self._mask\n",
        "\n",
        "\n",
        "class AnomalyAttention(nn.Module):\n",
        "    def __init__(self, win_size, mask_flag=True, scale=None, attention_dropout=0.0, output_attention=False):\n",
        "        super(AnomalyAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "        window_size = win_size\n",
        "        self.distances = torch.zeros((window_size, window_size)).cuda()\n",
        "        for i in range(window_size):\n",
        "            for j in range(window_size):\n",
        "                self.distances[i][j] = abs(i - j)\n",
        "\n",
        "    def forward(self, queries, keys, values, sigma, attn_mask):\n",
        "        B, L, H, E = queries.shape\n",
        "        _, S, _, D = values.shape\n",
        "        scale = self.scale or 1. / sqrt(E)\n",
        "\n",
        "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
        "        if self.mask_flag:\n",
        "            if attn_mask is None:\n",
        "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "        attn = scale * scores\n",
        "\n",
        "        sigma = sigma.transpose(1, 2)  # B L H ->  B H L\n",
        "        window_size = attn.shape[-1]\n",
        "        sigma = torch.sigmoid(sigma * 5) + 1e-5\n",
        "        sigma = torch.pow(3, sigma) - 1\n",
        "        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, window_size)  # B H L L\n",
        "        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1).cuda()\n",
        "        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2))\n",
        "\n",
        "        series = self.dropout(torch.softmax(attn, dim=-1))\n",
        "        V = torch.einsum(\"bhls,bshd->blhd\", series, values)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return (V.contiguous(), series, prior, sigma)\n",
        "        else:\n",
        "            return (V.contiguous(), None)\n",
        "\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
        "                 d_values=None):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "        d_keys = d_keys or (d_model // n_heads)\n",
        "        d_values = d_values or (d_model // n_heads)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.inner_attention = attention\n",
        "        self.query_projection = nn.Linear(d_model,\n",
        "                                          d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model,\n",
        "                                        d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model,\n",
        "                                          d_values * n_heads)\n",
        "        self.sigma_projection = nn.Linear(d_model,\n",
        "                                          n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "        x = queries\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "        sigma = self.sigma_projection(x).view(B, L, H)\n",
        "\n",
        "        out, series, prior, sigma = self.inner_attention(\n",
        "            queries,\n",
        "            keys,\n",
        "            values,\n",
        "            sigma,\n",
        "            attn_mask\n",
        "        )\n",
        "        out = out.view(B, L, -1)\n",
        "\n",
        "        return self.out_projection(out), series, prior, sigma\n"
      ],
      "metadata": {
        "id": "fB3pES0Ht7l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, dropout=0.0):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.value_embedding(x) + self.position_embedding(x)\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "6zKdDZqIuPJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        new_x, attn, mask, sigma = self.attention(\n",
        "            x, x, x,\n",
        "            attn_mask=attn_mask\n",
        "        )\n",
        "        x = x + self.dropout(new_x)\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "\n",
        "        return self.norm2(x + y), attn, mask, sigma\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, attn_layers, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # x [B, L, D]\n",
        "        series_list = []\n",
        "        prior_list = []\n",
        "        sigma_list = []\n",
        "        for attn_layer in self.attn_layers:\n",
        "            x, series, prior, sigma = attn_layer(x, attn_mask=attn_mask)\n",
        "            series_list.append(series)\n",
        "            prior_list.append(prior)\n",
        "            sigma_list.append(sigma)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x, series_list, prior_list, sigma_list\n",
        "\n",
        "\n",
        "class AnomalyTransformer(nn.Module):\n",
        "    def __init__(self, win_size, enc_in, c_out, d_model=512, n_heads=8, e_layers=3, d_ff=512,\n",
        "                 dropout=0.0, activation='gelu', output_attention=True):\n",
        "        super(AnomalyTransformer, self).__init__()\n",
        "        self.output_attention = output_attention\n",
        "\n",
        "        # Encoding\n",
        "        self.embedding = DataEmbedding(enc_in, d_model, dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AttentionLayer(\n",
        "                        AnomalyAttention(win_size, False, attention_dropout=dropout, output_attention=output_attention),\n",
        "                        d_model, n_heads),\n",
        "                    d_model,\n",
        "                    d_ff,\n",
        "                    dropout=dropout,\n",
        "                    activation=activation\n",
        "                ) for l in range(e_layers)\n",
        "            ],\n",
        "            norm_layer=torch.nn.LayerNorm(d_model)\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out = self.embedding(x)\n",
        "        enc_out, series, prior, sigmas = self.encoder(enc_out)\n",
        "        enc_out = self.projection(enc_out)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return enc_out, series, prior, sigmas\n",
        "        else:\n",
        "            return enc_out  # [B, L, D]\n"
      ],
      "metadata": {
        "id": "f90KU5m_tlxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_loader.py"
      ],
      "metadata": {
        "id": "w3mgBa_Xur60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class PSMSegLoader(object):\n",
        "    def __init__(self, data_path, win_size, step, mode=\"train\"):\n",
        "        self.mode = mode\n",
        "        self.step = step\n",
        "        self.win_size = win_size\n",
        "        self.scaler = StandardScaler()\n",
        "        data = pd.read_csv(data_path + '/train.csv')\n",
        "        data = data.values[:, 1:]\n",
        "\n",
        "        data = np.nan_to_num(data)\n",
        "\n",
        "        self.scaler.fit(data)\n",
        "        data = self.scaler.transform(data)\n",
        "        test_data = pd.read_csv(data_path + '/test.csv')\n",
        "\n",
        "        test_data = test_data.values[:, 1:]\n",
        "        test_data = np.nan_to_num(test_data)\n",
        "\n",
        "        self.test = self.scaler.transform(test_data)\n",
        "\n",
        "        self.train = data\n",
        "        self.val = self.test\n",
        "\n",
        "        self.test_labels = pd.read_csv(data_path + '/test_label.csv').values[:, 1:]\n",
        "\n",
        "        print(\"test:\", self.test.shape)\n",
        "        print(\"train:\", self.train.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of images in the object dataset.\n",
        "        \"\"\"\n",
        "        if self.mode == \"train\":\n",
        "            return (self.train.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'val'):\n",
        "            return (self.val.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'test'):\n",
        "            return (self.test.shape[0] - self.win_size) // self.step + 1\n",
        "        else:\n",
        "            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = index * self.step\n",
        "        if self.mode == \"train\":\n",
        "            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'val'):\n",
        "            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'test'):\n",
        "            return np.float32(self.test[index:index + self.win_size]), np.float32(\n",
        "                self.test_labels[index:index + self.win_size])\n",
        "        else:\n",
        "            return np.float32(self.test[\n",
        "                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n",
        "                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n",
        "\n",
        "\n",
        "class MSLSegLoader(object):\n",
        "    def __init__(self, data_path, win_size, step, mode=\"train\"):\n",
        "        self.mode = mode\n",
        "        self.step = step\n",
        "        self.win_size = win_size\n",
        "        self.scaler = StandardScaler()\n",
        "        data = np.load(data_path + \"/MSL_train.npy\")\n",
        "        self.scaler.fit(data)\n",
        "        data = self.scaler.transform(data)\n",
        "        test_data = np.load(data_path + \"/MSL_test.npy\")\n",
        "        self.test = self.scaler.transform(test_data)\n",
        "\n",
        "        self.train = data\n",
        "        self.val = self.test\n",
        "        self.test_labels = np.load(data_path + \"/MSL_test_label.npy\")\n",
        "        print(\"test:\", self.test.shape)\n",
        "        print(\"train:\", self.train.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            return (self.train.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'val'):\n",
        "            return (self.val.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'test'):\n",
        "            return (self.test.shape[0] - self.win_size) // self.step + 1\n",
        "        else:\n",
        "            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = index * self.step\n",
        "        if self.mode == \"train\":\n",
        "            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'val'):\n",
        "            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'test'):\n",
        "            return np.float32(self.test[index:index + self.win_size]), np.float32(\n",
        "                self.test_labels[index:index + self.win_size])\n",
        "        else:\n",
        "            return np.float32(self.test[\n",
        "                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n",
        "                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n",
        "\n",
        "\n",
        "class SMAPSegLoader(object):\n",
        "    def __init__(self, data_path, win_size, step, mode=\"train\"):\n",
        "        self.mode = mode\n",
        "        self.step = step\n",
        "        self.win_size = win_size\n",
        "        self.scaler = StandardScaler()\n",
        "        data = np.load(data_path + \"/SMAP_train.npy\")\n",
        "        self.scaler.fit(data)\n",
        "        data = self.scaler.transform(data)\n",
        "        test_data = np.load(data_path + \"/SMAP_test.npy\")\n",
        "        self.test = self.scaler.transform(test_data)\n",
        "\n",
        "        self.train = data\n",
        "        self.val = self.test\n",
        "        self.test_labels = np.load(data_path + \"/SMAP_test_label.npy\")\n",
        "        print(\"test:\", self.test.shape)\n",
        "        print(\"train:\", self.train.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            return (self.train.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'val'):\n",
        "            return (self.val.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'test'):\n",
        "            return (self.test.shape[0] - self.win_size) // self.step + 1\n",
        "        else:\n",
        "            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = index * self.step\n",
        "        if self.mode == \"train\":\n",
        "            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'val'):\n",
        "            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'test'):\n",
        "            return np.float32(self.test[index:index + self.win_size]), np.float32(\n",
        "                self.test_labels[index:index + self.win_size])\n",
        "        else:\n",
        "            return np.float32(self.test[\n",
        "                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n",
        "                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n",
        "\n",
        "\n",
        "class SMDSegLoader(object):\n",
        "    def __init__(self, data_path, win_size, step, mode=\"train\"):\n",
        "        self.mode = mode\n",
        "        self.step = step\n",
        "        self.win_size = win_size\n",
        "        self.scaler = StandardScaler()\n",
        "        data = np.load(data_path + \"/SMD_train.npy\")\n",
        "        self.scaler.fit(data)\n",
        "        data = self.scaler.transform(data)\n",
        "        test_data = np.load(data_path + \"/SMD_test.npy\")\n",
        "        self.test = self.scaler.transform(test_data)\n",
        "        self.train = data\n",
        "        data_len = len(self.train)\n",
        "        self.val = self.train[(int)(data_len * 0.8):]\n",
        "        self.test_labels = np.load(data_path + \"/SMD_test_label.npy\")\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            return (self.train.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'val'):\n",
        "            return (self.val.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'test'):\n",
        "            return (self.test.shape[0] - self.win_size) // self.step + 1\n",
        "        else:\n",
        "            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = index * self.step\n",
        "        if self.mode == \"train\":\n",
        "            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'val'):\n",
        "            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'test'):\n",
        "            return np.float32(self.test[index:index + self.win_size]), np.float32(\n",
        "                self.test_labels[index:index + self.win_size])\n",
        "        else:\n",
        "            return np.float32(self.test[\n",
        "                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n",
        "                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n",
        "\n",
        "\n",
        "def get_loader_segment(data_path, batch_size, win_size=100, step=100, mode='train', dataset='KDD'):\n",
        "    if (dataset == 'SMD'):\n",
        "        dataset = SMDSegLoader(data_path, win_size, step, mode)\n",
        "    elif (dataset == 'MSL'):\n",
        "        dataset = MSLSegLoader(data_path, win_size, 1, mode)\n",
        "    elif (dataset == 'SMAP'):\n",
        "        dataset = SMAPSegLoader(data_path, win_size, 1, mode)\n",
        "    elif (dataset == 'PSM'):\n",
        "        dataset = PSMSegLoader(data_path, win_size, 1, mode)\n",
        "\n",
        "    shuffle = False\n",
        "    if mode == 'train':\n",
        "        shuffle = True\n",
        "\n",
        "    data_loader = DataLoader(dataset=dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=0)\n",
        "    return data_loader\n"
      ],
      "metadata": {
        "id": "XbJDXMSeun9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_kl_loss(p, q):\n",
        "    res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
        "    return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr_):\n",
        "    lr_adjust = {epoch: lr_ * (0.5 ** ((epoch - 1) // 1))}\n",
        "    if epoch in lr_adjust.keys():\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print('Updating learning rate to {}'.format(lr))\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, dataset_name='', delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.best_score2 = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.val_loss2_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.dataset = dataset_name\n",
        "\n",
        "    def __call__(self, val_loss, val_loss2, model, path):\n",
        "        score = -val_loss\n",
        "        score2 = -val_loss2\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_score2 = score2\n",
        "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
        "        elif score < self.best_score + self.delta or score2 < self.best_score2 + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_score2 = score2\n",
        "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, val_loss2, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), os.path.join(path, str(self.dataset) + '_checkpoint.pth'))\n",
        "        self.val_loss_min = val_loss\n",
        "        self.val_loss2_min = val_loss2\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    DEFAULTS = {}\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        self.__dict__.update(Solver.DEFAULTS, **config)\n",
        "\n",
        "        self.train_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                               mode='train',\n",
        "                                               dataset=self.dataset)\n",
        "        self.vali_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='val',\n",
        "                                              dataset=self.dataset)\n",
        "        self.test_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='test',\n",
        "                                              dataset=self.dataset)\n",
        "        self.thre_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='thre',\n",
        "                                              dataset=self.dataset)\n",
        "\n",
        "        self.build_model()\n",
        "        self.device = torch.device(\"mps\")   # self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.model = AnomalyTransformer(win_size=self.win_size, enc_in=self.input_c, c_out=self.output_c, e_layers=3)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        # if torch.cuda.is_available():\n",
        "        #     self.model.cuda()\n",
        "\n",
        "    def vali(self, vali_loader):\n",
        "        self.model.eval()\n",
        "\n",
        "        loss_1 = []\n",
        "        loss_2 = []\n",
        "        for i, (input_data, _) in enumerate(vali_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                series_loss += (torch.mean(my_kl_loss(series[u], (\n",
        "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                               self.win_size)).detach())) + torch.mean(\n",
        "                    my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)).detach(),\n",
        "                        series[u])))\n",
        "                prior_loss += (torch.mean(\n",
        "                    my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)),\n",
        "                               series[u].detach())) + torch.mean(\n",
        "                    my_kl_loss(series[u].detach(),\n",
        "                               (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)))))\n",
        "            series_loss = series_loss / len(prior)\n",
        "            prior_loss = prior_loss / len(prior)\n",
        "\n",
        "            rec_loss = self.criterion(output, input)\n",
        "            loss_1.append((rec_loss - self.k * series_loss).item())\n",
        "            loss_2.append((rec_loss + self.k * prior_loss).item())\n",
        "\n",
        "        return np.average(loss_1), np.average(loss_2)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        print(\"======================TRAIN MODE======================\")\n",
        "\n",
        "        time_now = time.time()\n",
        "        path = self.model_save_path\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        early_stopping = EarlyStopping(patience=3, verbose=True, dataset_name=self.dataset)\n",
        "        train_steps = len(self.train_loader)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            iter_count = 0\n",
        "            loss1_list = []\n",
        "\n",
        "            epoch_time = time.time()\n",
        "            self.model.train()\n",
        "            for i, (input_data, labels) in enumerate(self.train_loader):\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                iter_count += 1\n",
        "                input = input_data.float().to(self.device)\n",
        "\n",
        "                output, series, prior, _ = self.model(input)\n",
        "\n",
        "                # calculate Association discrepancy\n",
        "                series_loss = 0.0\n",
        "                prior_loss = 0.0\n",
        "                for u in range(len(prior)):\n",
        "                    series_loss += (torch.mean(my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach())) + torch.mean(\n",
        "                        my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                           self.win_size)).detach(),\n",
        "                                   series[u])))\n",
        "                    prior_loss += (torch.mean(my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach())) + torch.mean(\n",
        "                        my_kl_loss(series[u].detach(), (\n",
        "                                prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)))))\n",
        "                series_loss = series_loss / len(prior)\n",
        "                prior_loss = prior_loss / len(prior)\n",
        "\n",
        "                rec_loss = self.criterion(output, input)\n",
        "\n",
        "                loss1_list.append((rec_loss - self.k * series_loss).item())\n",
        "                loss1 = rec_loss - self.k * series_loss\n",
        "                loss2 = rec_loss + self.k * prior_loss\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    speed = (time.time() - time_now) / iter_count\n",
        "                    left_time = speed * ((self.num_epochs - epoch) * train_steps - i)\n",
        "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
        "                    iter_count = 0\n",
        "                    time_now = time.time()\n",
        "\n",
        "                # Minimax strategy\n",
        "                loss1.backward(retain_graph=True)\n",
        "                loss2.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
        "            train_loss = np.average(loss1_list)\n",
        "\n",
        "            vali_loss1, vali_loss2 = self.vali(self.test_loader)\n",
        "\n",
        "            print(\n",
        "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} \".format(\n",
        "                    epoch + 1, train_steps, train_loss, vali_loss1))\n",
        "            early_stopping(vali_loss1, vali_loss2, self.model, path)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "            adjust_learning_rate(self.optimizer, epoch + 1, self.lr)\n",
        "\n",
        "    def test(self):\n",
        "        self.model.load_state_dict(\n",
        "            torch.load(\n",
        "                os.path.join(str(self.model_save_path), str(self.dataset) + '_checkpoint.pth')))\n",
        "        self.model.eval()\n",
        "        temperature = 50\n",
        "\n",
        "        print(\"======================TEST MODE======================\")\n",
        "\n",
        "        criterion = nn.MSELoss(reduce=False)\n",
        "\n",
        "        # (1) stastic on the train set\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.train_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        train_energy = np.array(attens_energy)\n",
        "\n",
        "        # (2) find the threshold\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "            # Metric\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        test_energy = np.array(attens_energy)\n",
        "        combined_energy = np.concatenate([train_energy, test_energy], axis=0)\n",
        "        thresh = np.percentile(combined_energy, 100 - self.anormly_ratio)\n",
        "        print(\"Threshold :\", thresh)\n",
        "\n",
        "        # (3) evaluation on the test set\n",
        "        test_labels = []\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "            test_labels.append(labels)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        test_labels = np.concatenate(test_labels, axis=0).reshape(-1)\n",
        "        test_energy = np.array(attens_energy)\n",
        "        test_labels = np.array(test_labels)\n",
        "\n",
        "        pred = (test_energy > thresh).astype(int)\n",
        "\n",
        "        gt = test_labels.astype(int)\n",
        "\n",
        "        print(\"pred:   \", pred.shape)\n",
        "        print(\"gt:     \", gt.shape)\n",
        "\n",
        "        # detection adjustment\n",
        "        anomaly_state = False\n",
        "        for i in range(len(gt)):\n",
        "            if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
        "                anomaly_state = True\n",
        "                for j in range(i, 0, -1):\n",
        "                    if gt[j] == 0:\n",
        "                        break\n",
        "                    else:\n",
        "                        if pred[j] == 0:\n",
        "                            pred[j] = 1\n",
        "                for j in range(i, len(gt)):\n",
        "                    if gt[j] == 0:\n",
        "                        break\n",
        "                    else:\n",
        "                        if pred[j] == 0:\n",
        "                            pred[j] = 1\n",
        "            elif gt[i] == 0:\n",
        "                anomaly_state = False\n",
        "            if anomaly_state:\n",
        "                pred[i] = 1\n",
        "\n",
        "        pred = np.array(pred)\n",
        "        gt = np.array(gt)\n",
        "        print(\"pred: \", pred.shape)\n",
        "        print(\"gt:   \", gt.shape)\n",
        "\n",
        "        from sklearn.metrics import precision_recall_fscore_support\n",
        "        from sklearn.metrics import accuracy_score\n",
        "        accuracy = accuracy_score(gt, pred)\n",
        "        precision, recall, f_score, support = precision_recall_fscore_support(gt, pred,\n",
        "                                                                              average='binary')\n",
        "        print(\n",
        "            \"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f} \".format(\n",
        "                accuracy, precision,\n",
        "                recall, f_score))\n",
        "\n",
        "        return accuracy, precision, recall, f_score\n"
      ],
      "metadata": {
        "id": "LOAN6AkXtaIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def my_kl_loss(p, q):\n",
        "    res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
        "    return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr_):\n",
        "    lr_adjust = {epoch: lr_ * (0.5 ** ((epoch - 1) // 1))}\n",
        "    if epoch in lr_adjust.keys():\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print('Updating learning rate to {}'.format(lr))\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, dataset_name='', delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.best_score2 = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.val_loss2_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.dataset = dataset_name\n",
        "\n",
        "    def __call__(self, val_loss, val_loss2, model, path):\n",
        "        score = -val_loss\n",
        "        score2 = -val_loss2\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_score2 = score2\n",
        "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
        "        elif score < self.best_score + self.delta or score2 < self.best_score2 + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_score2 = score2\n",
        "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, val_loss2, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), os.path.join(path, str(self.dataset) + '_checkpoint.pth'))\n",
        "        self.val_loss_min = val_loss\n",
        "        self.val_loss2_min = val_loss2\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    DEFAULTS = {}\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        self.__dict__.update(Solver.DEFAULTS, **config)\n",
        "\n",
        "        self.train_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                               mode='train',\n",
        "                                               dataset=self.dataset)\n",
        "        self.vali_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='val',\n",
        "                                              dataset=self.dataset)\n",
        "        self.test_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='test',\n",
        "                                              dataset=self.dataset)\n",
        "        self.thre_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='thre',\n",
        "                                              dataset=self.dataset)\n",
        "\n",
        "        self.build_model()\n",
        "        self.device = torch.device(\"mps\")   # self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.model = AnomalyTransformer(win_size=self.win_size, enc_in=self.input_c, c_out=self.output_c, e_layers=3)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        # if torch.cuda.is_available():\n",
        "        #     self.model.cuda()\n",
        "\n",
        "    def vali(self, vali_loader):\n",
        "        self.model.eval()\n",
        "\n",
        "        loss_1 = []\n",
        "        loss_2 = []\n",
        "        for i, (input_data, _) in enumerate(vali_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                series_loss += (torch.mean(my_kl_loss(series[u], (\n",
        "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                               self.win_size)).detach())) + torch.mean(\n",
        "                    my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)).detach(),\n",
        "                        series[u])))\n",
        "                prior_loss += (torch.mean(\n",
        "                    my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)),\n",
        "                               series[u].detach())) + torch.mean(\n",
        "                    my_kl_loss(series[u].detach(),\n",
        "                               (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)))))\n",
        "            series_loss = series_loss / len(prior)\n",
        "            prior_loss = prior_loss / len(prior)\n",
        "\n",
        "            rec_loss = self.criterion(output, input)\n",
        "            loss_1.append((rec_loss - self.k * series_loss).item())\n",
        "            loss_2.append((rec_loss + self.k * prior_loss).item())\n",
        "\n",
        "        return np.average(loss_1), np.average(loss_2)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        print(\"======================TRAIN MODE======================\")\n",
        "\n",
        "        time_now = time.time()\n",
        "        path = self.model_save_path\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        early_stopping = EarlyStopping(patience=3, verbose=True, dataset_name=self.dataset)\n",
        "        train_steps = len(self.train_loader)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            iter_count = 0\n",
        "            loss1_list = []\n",
        "\n",
        "            epoch_time = time.time()\n",
        "            self.model.train()\n",
        "            for i, (input_data, labels) in enumerate(self.train_loader):\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                iter_count += 1\n",
        "                input = input_data.float().to(self.device)\n",
        "\n",
        "                output, series, prior, _ = self.model(input)\n",
        "\n",
        "                # calculate Association discrepancy\n",
        "                series_loss = 0.0\n",
        "                prior_loss = 0.0\n",
        "                for u in range(len(prior)):\n",
        "                    series_loss += (torch.mean(my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach())) + torch.mean(\n",
        "                        my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                           self.win_size)).detach(),\n",
        "                                   series[u])))\n",
        "                    prior_loss += (torch.mean(my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach())) + torch.mean(\n",
        "                        my_kl_loss(series[u].detach(), (\n",
        "                                prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)))))\n",
        "                series_loss = series_loss / len(prior)\n",
        "                prior_loss = prior_loss / len(prior)\n",
        "\n",
        "                rec_loss = self.criterion(output, input)\n",
        "\n",
        "                loss1_list.append((rec_loss - self.k * series_loss).item())\n",
        "                loss1 = rec_loss - self.k * series_loss\n",
        "                loss2 = rec_loss + self.k * prior_loss\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    speed = (time.time() - time_now) / iter_count\n",
        "                    left_time = speed * ((self.num_epochs - epoch) * train_steps - i)\n",
        "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
        "                    iter_count = 0\n",
        "                    time_now = time.time()\n",
        "\n",
        "                # Minimax strategy\n",
        "                loss1.backward(retain_graph=True)\n",
        "                loss2.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
        "            train_loss = np.average(loss1_list)\n",
        "\n",
        "            vali_loss1, vali_loss2 = self.vali(self.test_loader)\n",
        "\n",
        "            print(\n",
        "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} \".format(\n",
        "                    epoch + 1, train_steps, train_loss, vali_loss1))\n",
        "            early_stopping(vali_loss1, vali_loss2, self.model, path)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "            adjust_learning_rate(self.optimizer, epoch + 1, self.lr)\n",
        "\n",
        "    def test(self):\n",
        "        self.model.load_state_dict(\n",
        "            torch.load(\n",
        "                os.path.join(str(self.model_save_path), str(self.dataset) + '_checkpoint.pth')))\n",
        "        self.model.eval()\n",
        "        temperature = 50\n",
        "\n",
        "        print(\"======================TEST MODE======================\")\n",
        "\n",
        "        criterion = nn.MSELoss(reduce=False)\n",
        "\n",
        "        # (1) stastic on the train set\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.train_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        train_energy = np.array(attens_energy)\n",
        "\n",
        "        # (2) find the threshold\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "            # Metric\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        test_energy = np.array(attens_energy)\n",
        "        combined_energy = np.concatenate([train_energy, test_energy], axis=0)\n",
        "        thresh = np.percentile(combined_energy, 100 - self.anormly_ratio)\n",
        "        print(\"Threshold :\", thresh)\n",
        "\n",
        "        # (3) evaluation on the test set\n",
        "        test_labels = []\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "            test_labels.append(labels)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        test_labels = np.concatenate(test_labels, axis=0).reshape(-1)\n",
        "        test_energy = np.array(attens_energy)\n",
        "        test_labels = np.array(test_labels)\n",
        "\n",
        "        pred = (test_energy > thresh).astype(int)\n",
        "\n",
        "        gt = test_labels.astype(int)\n",
        "\n",
        "        print(\"pred:   \", pred.shape)\n",
        "        print(\"gt:     \", gt.shape)\n",
        "\n",
        "        # detection adjustment\n",
        "        anomaly_state = False\n",
        "        for i in range(len(gt)):\n",
        "            if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
        "                anomaly_state = True\n",
        "                for j in range(i, 0, -1):\n",
        "                    if gt[j] == 0:\n",
        "                        break\n",
        "                    else:\n",
        "                        if pred[j] == 0:\n",
        "                            pred[j] = 1\n",
        "                for j in range(i, len(gt)):\n",
        "                    if gt[j] == 0:\n",
        "                        break\n",
        "                    else:\n",
        "                        if pred[j] == 0:\n",
        "                            pred[j] = 1\n",
        "            elif gt[i] == 0:\n",
        "                anomaly_state = False\n",
        "            if anomaly_state:\n",
        "                pred[i] = 1\n",
        "\n",
        "        pred = np.array(pred)\n",
        "        gt = np.array(gt)\n",
        "        print(\"pred: \", pred.shape)\n",
        "        print(\"gt:   \", gt.shape)\n",
        "\n",
        "        from sklearn.metrics import precision_recall_fscore_support\n",
        "        from sklearn.metrics import accuracy_score\n",
        "        accuracy = accuracy_score(gt, pred)\n",
        "        precision, recall, f_score, support = precision_recall_fscore_support(gt, pred,\n",
        "                                                                              average='binary')\n",
        "        print(\n",
        "            \"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f} \".format(\n",
        "                accuracy, precision,\n",
        "                recall, f_score))\n",
        "\n",
        "        return accuracy, precision, recall, f_score\n"
      ],
      "metadata": {
        "id": "1XwXajjiveGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "# 定义辅助函数str2bool(v)，用于将字符串输入转换为布尔值\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n"
      ],
      "metadata": {
        "id": "tpzWDZjsvMfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "metadata": {
        "id": "mjxuGxRvzqG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "# main(config)函数是脚本的核心。它接受一个配置对象作为参数，\n",
        "# 该对象包含运行程序所需的所有参数。它检查模型保存路径是否存在，如果不存在则创建目录。\n",
        "# 然后它使用提供的配置创建一个Solver类的实例，并根据配置中指定的模式调用Solver实例的train()或test()方法。\n",
        "\n",
        "# 如果直接运行脚本（而不是作为模块导入），它会定义一个命令行参数解析器。\n",
        "\n",
        "# 这允许用户在从命令行运行脚本时指定各种参数，例如学习率（--lr），训练的周期数（--num_epochs），批处理大小（--batch_size），\n",
        "# 输入通道大小（--input_c），输出通道大小（--output_c），要使用的数据集（--dataset），操作模式（--mode），数据路径（--data_path），\n",
        "# 保存模型的路径（--model_save_path），以及异常比例（--anormly_ratio）\n",
        "\n",
        "# argparse.ArgumentParser()类用于处理命令行参数。\n",
        "# add_argument()方法用于指定程序期望的命令行选项。\n",
        "# 在这个脚本中，用户可以指定模型训练和测试的参数。\n",
        "# config = parser.parse_args()这行代码解析命令行参数，并将结果存储在config变量中。\n",
        "# 解析命令行参数后，将它们打印到控制台，以供用户确认。\n",
        "# 最后，使用解析后的命令行参数调用main()函数。\n",
        "\n",
        "def main(config):\n",
        "    # cuDNN库是NVIDIA为深度神经网络提供的一个GPU加速库，它包含了很多深度学习中常用的标准算法。\n",
        "    # 当你的网络的输入数据维度或类型上不会发生改变的时候，使用cudnn.benchmark = True\n",
        "    # 通常可以显著提升运行效率，因为cuDNN会在程序开始时花费一些额外的时间进行优化，然后在之后的运算中复用这个优化结果。\n",
        "    # cudnn.benchmark = True\n",
        "    if (not os.path.exists(config.model_save_path)):\n",
        "        mkdir(config.model_save_path)\n",
        "    solver = Solver(vars(config))\n",
        "\n",
        "    if config.mode == 'train':\n",
        "        solver.train()\n",
        "    elif config.mode == 'test':\n",
        "        solver.test()\n",
        "\n",
        "    return solver\n"
      ],
      "metadata": {
        "id": "WItV2kJkwh-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "KDHJiBqdpSbd",
        "outputId": "0d0f4cec-1d7f-4c80-fa77-3efa958599f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--lr LR] [--num_epochs NUM_EPOCHS] [--k K]\n",
            "                             [--win_size WIN_SIZE] [--input_c INPUT_C]\n",
            "                             [--output_c OUTPUT_C] [--batch_size BATCH_SIZE]\n",
            "                             [--pretrained_model PRETRAINED_MODEL]\n",
            "                             [--dataset DATASET] [--mode {train,test}]\n",
            "                             [--data_path DATA_PATH]\n",
            "                             [--model_save_path MODEL_SAVE_PATH]\n",
            "                             [--anormly_ratio ANORMLY_RATIO]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-b2d6f2d7-21a8-4afe-a37d-d9e40556bc29.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--num_epochs', type=int, default=10)\n",
        "    parser.add_argument('--k', type=int, default=3)\n",
        "    parser.add_argument('--win_size', type=int, default=100)\n",
        "    parser.add_argument('--input_c', type=int, default=38)\n",
        "    parser.add_argument('--output_c', type=int, default=38)\n",
        "    parser.add_argument('--batch_size', type=int, default=1024)\n",
        "    parser.add_argument('--pretrained_model', type=str, default=None)\n",
        "    parser.add_argument('--dataset', type=str, default='credit')\n",
        "    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
        "    parser.add_argument('--data_path', type=str, default='./dataset/creditcard_ts.csv')\n",
        "    parser.add_argument('--model_save_path', type=str, default='checkpoints')\n",
        "    parser.add_argument('--anormly_ratio', type=float, default=4.00)\n",
        "\n",
        "    config = parser.parse_args()\n",
        "\n",
        "    # args = vars(config)\n",
        "    # print('------------ Options -------------')\n",
        "    # for k, v in sorted(args.items()):\n",
        "    #     print('%s: %s' % (str(k), str(v)))\n",
        "    # print('-------------- End ----------------')\n",
        "    # main(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "python main.py --anormly_ratio 1 --num_epochs 3   --batch_size 256  --mode train --dataset SMAP  --data_path dataset/SMAP --input_c 25    --output_c 25\n",
        "python main.py --anormly_ratio 1  --num_epochs 10        --batch_size 256     --mode test    --dataset SMAP   --data_path dataset/SMAP  --input_c 25    --output_c 25  --pretrained_model 20\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "wi8zc5HK3TZ8",
        "outputId": "35ace562-fb94-46a6-d3b9-4b072ba5edca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-031f03a87814>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    python main.py --anormly_ratio 1 --num_epochs 3   --batch_size 256  --mode train --dataset SMAP  --data_path dataset/SMAP --input_c 25    --output_c 25\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/SMAP.sh。/content/drive/MyDrive/Getting started.pdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD2n24IO5d3_",
        "outputId": "47efd5bb-f7d2-41d7-94b1-a32e210da540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/SMAP.sh\", line 3\n",
            "    python main.py --anormly_ratio 1 --num_epochs 3   --batch_size 256  --mode train --dataset SMAP  --data_path dataset/SMAP --input_c 25    --output_c 25\n",
            "           ^^^^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash SMAP.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlLeqZSPB7R_",
        "outputId": "8acc35be-c78e-4571-c7df-e46356c5c380"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "anormly_ratio: 1.0\n",
            "batch_size: 256\n",
            "data_path: dataset/SMAP\n",
            "dataset: SMAP\n",
            "input_c: 25\n",
            "k: 3\n",
            "lr: 0.0001\n",
            "mode: train\n",
            "model_save_path: checkpoints\n",
            "num_epochs: 3\n",
            "output_c: 25\n",
            "pretrained_model: None\n",
            "win_size: 100\n",
            "-------------- End ----------------\n",
            "test: (427617, 25)\n",
            "train: (135183, 25)\n",
            "test: (427617, 25)\n",
            "train: (135183, 25)\n",
            "test: (427617, 25)\n",
            "train: (135183, 25)\n",
            "test: (427617, 25)\n",
            "train: (135183, 25)\n",
            "======================TRAIN MODE======================\n",
            "\tspeed: 0.2272s/iter; left time: 337.3328s\n",
            "\tspeed: 0.2145s/iter; left time: 297.0993s\n",
            "\tspeed: 0.2148s/iter; left time: 275.9659s\n",
            "\tspeed: 0.2148s/iter; left time: 254.5411s\n",
            "\tspeed: 0.2151s/iter; left time: 233.3954s\n",
            "Epoch: 1 cost time: 114.64892959594727\n",
            "Epoch: 1, Steps: 528 | Train Loss: -44.7279403 Vali Loss: -47.3299936 \n",
            "Validation loss decreased (inf --> -47.329994).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\tspeed: 1.4166s/iter; left time: 1355.6687s\n",
            "\tspeed: 0.2149s/iter; left time: 184.1314s\n",
            "\tspeed: 0.2150s/iter; left time: 162.7591s\n",
            "\tspeed: 0.2148s/iter; left time: 141.1552s\n",
            "\tspeed: 0.2148s/iter; left time: 119.6360s\n",
            "Epoch: 2 cost time: 113.3216814994812\n",
            "Epoch: 2, Steps: 528 | Train Loss: -48.0040937 Vali Loss: -47.8762193 \n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "\tspeed: 1.4155s/iter; left time: 607.2519s\n",
            "\tspeed: 0.2147s/iter; left time: 70.6313s\n",
            "\tspeed: 0.2148s/iter; left time: 49.1872s\n",
            "\tspeed: 0.2148s/iter; left time: 27.7064s\n",
            "\tspeed: 0.2146s/iter; left time: 6.2229s\n",
            "Epoch: 3 cost time: 113.24240255355835\n",
            "Epoch: 3, Steps: 528 | Train Loss: -48.2526774 Vali Loss: -47.9804853 \n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "------------ Options -------------\n",
            "anormly_ratio: 1.0\n",
            "batch_size: 256\n",
            "data_path: dataset/SMAP\n",
            "dataset: SMAP\n",
            "input_c: 25\n",
            "k: 3\n",
            "lr: 0.0001\n",
            "mode: test\n",
            "model_save_path: checkpoints\n",
            "num_epochs: 10\n",
            "output_c: 25\n",
            "pretrained_model: 20\n",
            "win_size: 100\n",
            "-------------- End ----------------\n",
            "test: (427617, 25)\n",
            "train: (135183, 25)\n",
            "test: (427617, 25)\n",
            "train: (135183, 25)\n",
            "test: (427617, 25)\n",
            "train: (135183, 25)\n",
            "test: (427617, 25)\n",
            "train: (135183, 25)\n",
            "======================TEST MODE======================\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Threshold : 0.0009982840856537223\n",
            "pred:    (427600,)\n",
            "gt:      (427600,)\n",
            "pred:  (427600,)\n",
            "gt:    (427600,)\n",
            "Accuracy : 0.9908, Precision : 0.9387, Recall : 0.9926, F-score : 0.9649 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash Hz.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlJausl8-QZa",
        "outputId": "93482610-230b-4afd-b28f-72095cf70efc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "anormly_ratio: 1.0\n",
            "batch_size: 256\n",
            "data_path: dataset/Hz\n",
            "dataset: Hz\n",
            "input_c: 1\n",
            "k: 3\n",
            "lr: 0.0001\n",
            "mode: train\n",
            "model_save_path: checkpoints\n",
            "num_epochs: 3\n",
            "output_c: 1\n",
            "pretrained_model: None\n",
            "win_size: 100\n",
            "-------------- End ----------------\n",
            "test: (2202, 1)\n",
            "train: (1538, 1)\n",
            "test: (2202, 1)\n",
            "train: (1538, 1)\n",
            "test: (2202, 1)\n",
            "train: (1538, 1)\n",
            "test: (2202, 1)\n",
            "train: (1538, 1)\n",
            "======================TRAIN MODE======================\n",
            "Epoch: 1 cost time: 1.4972343444824219\n",
            "Epoch: 1, Steps: 6 | Train Loss: -22.2506393 Vali Loss: -20.4380781 \n",
            "Validation loss decreased (inf --> -20.438078).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 1.1233093738555908\n",
            "Epoch: 2, Steps: 6 | Train Loss: -22.4099464 Vali Loss: -22.4145080 \n",
            "Validation loss decreased (-20.438078 --> -22.414508).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 1.1230995655059814\n",
            "Epoch: 3, Steps: 6 | Train Loss: -22.9837001 Vali Loss: -22.5006652 \n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "------------ Options -------------\n",
            "anormly_ratio: 1.0\n",
            "batch_size: 256\n",
            "data_path: dataset/Hz\n",
            "dataset: Hz\n",
            "input_c: 1\n",
            "k: 3\n",
            "lr: 0.0001\n",
            "mode: test\n",
            "model_save_path: checkpoints\n",
            "num_epochs: 10\n",
            "output_c: 1\n",
            "pretrained_model: 20\n",
            "win_size: 100\n",
            "-------------- End ----------------\n",
            "test: (2202, 1)\n",
            "train: (1538, 1)\n",
            "test: (2202, 1)\n",
            "train: (1538, 1)\n",
            "test: (2202, 1)\n",
            "train: (1538, 1)\n",
            "test: (2202, 1)\n",
            "train: (1538, 1)\n",
            "======================TEST MODE======================\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Threshold : 0.10937310107052595\n",
            "pred:    (2200,)\n",
            "gt:      (2200,)\n",
            "pred:  (2200,)\n",
            "gt:    (2200,)\n",
            "Accuracy : 0.9927, Precision : 0.8699, Recall : 1.0000, F-score : 0.9304 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fWloAFfCfqjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/dataset/10Hz/10Hz_label.csv')\n",
        "data2 = data.values\n",
        "print(data2[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxOLRp47_pGi",
        "outputId": "e8b61fac-95d5-4276-ba5c-eb56196b6412"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2iIqOTWGruz",
        "outputId": "813df162-7524-4f78-f8b7-2797b3c7c25b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2202, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = data2.astype(bool)"
      ],
      "metadata": {
        "id": "W4aV0AEvAHId"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data3)\n",
        "df.to_csv('/content/dataset/10Hz/10Hz_label.csv', index=False) "
      ],
      "metadata": {
        "id": "m2rwcgjzG5ST"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}